{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'user_behavior' deleted successfully.\n"
     ]
    },
    {
     "ename": "TopicAlreadyExistsError",
     "evalue": "[Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='user_behavior', num_partitions=1, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='user_behavior', error_code=36, error_message=\"Topic 'user_behavior' already exists.\")])'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# create topic foobar with N partitions\u001b[39;00m\n\u001b[1;32m     22\u001b[0m admin_client \u001b[38;5;241m=\u001b[39m KafkaAdminClient(bootstrap_servers\u001b[38;5;241m=\u001b[39mbootstrap_servers)\n\u001b[0;32m---> 23\u001b[0m \u001b[43madmin_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mNewTopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopic_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_PARTITIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplication_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# get number of partitions\u001b[39;00m\n\u001b[1;32m     27\u001b[0m admin_client\u001b[38;5;241m.\u001b[39mdescribe_topics(topic_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigdata/lib/python3.9/site-packages/kafka/admin/client.py:461\u001b[0m, in \u001b[0;36mKafkaAdminClient.create_topics\u001b[0;34m(self, new_topics, timeout_ms, validate_only)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupport for CreateTopics v\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m has not yet been added to KafkaAdminClient.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;241m.\u001b[39mformat(version))\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# TODO convert structs to a more pythonic interface\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# TODO raise exceptions if errors\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request_to_controller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigdata/lib/python3.9/site-packages/kafka/admin/client.py:407\u001b[0m, in \u001b[0;36mKafkaAdminClient._send_request_to_controller\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m error_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoError:\n\u001b[0;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error_type(\n\u001b[1;32m    408\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m failed with response \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m             \u001b[38;5;241m.\u001b[39mformat(request, response))\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mTopicAlreadyExistsError\u001b[0m: [Error 36] TopicAlreadyExistsError: Request 'CreateTopicsRequest_v3(create_topic_requests=[(topic='user_behavior', num_partitions=1, replication_factor=1, replica_assignment=[], configs=[])], timeout=30000, validate_only=False)' failed with response 'CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='user_behavior', error_code=36, error_message=\"Topic 'user_behavior' already exists.\")])'."
     ]
    }
   ],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "# Example usage\n",
    "topic_name = 'output-topic'\n",
    "bootstrap_servers = [\"10.237.96.122:9092\"]\n",
    "N_PARTITIONS = 1\n",
    "def delete_kafka_topic(topic_name, bootstrap_servers=bootstrap_servers):\n",
    "    # Create an admin client\n",
    "    admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers[0])\n",
    "\n",
    "    # Delete the topic\n",
    "    admin_client.delete_topics([topic_name])\n",
    "    print(f\"Topic '{topic_name}' deleted successfully.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    delete_kafka_topic(topic_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# create topic foobar with N partitions\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "admin_client.create_topics([NewTopic(name=topic_name, num_partitions=N_PARTITIONS, replication_factor=1)])\n",
    "\n",
    "\n",
    "# get number of partitions\n",
    "admin_client.describe_topics(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicPartition(topic='input-topic', partition=0) [ConsumerRecord(topic='input-topic', partition=0, offset=2, timestamp=1732445632071, timestamp_type=0, key=None, value=b\"# Setup\\n```sh\\nwget https://raw.githubusercontent.com/confluentinc/cp-all-in-one/refs/heads/7.5.0-post/cp-all-in-one-flink/docker-compose.yml\\ndocker compose up -d\\ndocker exec -it flink-sql-client sql-client.sh\\n```\\n\\n# Tunel\\n```sh\\n# list topic\\ndocker exec -it broker kafka-topics --bootstrap-server broker:29092 --list\\n```\\n# Usage\\n## create topic\\n```sql\\nDROP TABLE IF EXISTS KafkaTable;\\nCREATE TABLE KafkaTable (\\n  `user_id` BIGINT,\\n  `item_id` BIGINT,\\n  `behavior` STRING,\\n  `ts` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'user_behavior',\\n  'properties.bootstrap.servers' = 'localhost:9092',\\n  'properties.group.id' = 'testGroup',\\n  'scan.startup.mode' = 'earliest-offset',\\n  'format' = 'csv'\\n);\\nSELECT * FROM KafkaTable;\\n```\\n\\n## Upsert\\n```sql\\nCREATE TABLE pageviews_per_region (\\n  user_region STRING,\\n  pv BIGINT,\\n  uv BIGINT,\\n  PRIMARY KEY (user_region) NOT ENFORCED\\n) WITH (\\n  'connector' = 'upsert-kafka',\\n  'topic' = 'pageviews_per_region',\\n  'properties.bootstrap.servers' = '...',\\n  'key.format' = 'avro',\\n  'value.format' = 'avro'\\n);\\n\\nCREATE TABLE pageviews (\\n  user_id BIGINT,\\n  page_id BIGINT,\\n  viewtime TIMESTAMP,\\n  user_region STRING,\\n  WATERMARK FOR viewtime AS viewtime - INTERVAL '2' SECOND\\n) WITH (\\n  'connector' = 'kafka',\\n  'topic' = 'pageviews',\\n  'properties.bootstrap.servers' = '...',\\n  'format' = 'json'\\n);\\n\\n-- calculate the pv, uv and insert into the upsert-kafka sink\\nINSERT INTO pageviews_per_region\\nSELECT\\n  user_region,\\n  COUNT(*),\\n  COUNT(DISTINCT user_id)\\nFROM pageviews\\nGROUP BY user_region;\\n\\n```\\n\\n# Detail of service and configuration\\n## tunnel\\n```sh\\nssh -N -L 9081:localhost:9081 demo@10.237.96.122\\n```\\n## service\\n| service | port | url | note |\\n| --- | --- | --- | --- |\\n| flink dashboard | 9081 | http://localhost:9081 | |\\n| kafka control-center | 9021 | http://localhost:9021 | |\\n| kafka broker | 9092 | | |\", headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=1889, serialized_header_size=-1)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "# Create a KafkaConsumer instance\n",
    "consumer = KafkaConsumer(\n",
    "    # bootstrap_servers=['localhost:9092'],\n",
    "    bootstrap_servers=[\"10.237.96.122:9092\"],\n",
    "    auto_offset_reset='latest',\n",
    "    # auto_offset_reset='earliest',\n",
    "    # receive_buffer_bytes=160*1024,\n",
    "    enable_auto_commit=False,\n",
    "    # value_deserializer=lambda m: json.loads(m.decode('ascii')),\n",
    ")\n",
    "\n",
    "# Subscribe to a specific topic\n",
    "# TOPIC_NAME = \"input-topic\"\n",
    "TOPIC_NAME = \"output-topic\"\n",
    "consumer.subscribe(topics=[TOPIC_NAME])\n",
    "N_sample =  - 1\n",
    "while True:\n",
    "    msg_poll = consumer.poll(timeout_ms=0)\n",
    "    for k, v in msg_poll.items():\n",
    "        print(k, v)\n",
    "    # break\n",
    "\n",
    "# df_record = pd.DataFrame(lst_record)\n",
    "# df_record.describe()\n",
    "msg_poll = consumer.poll(timeout_ms=0)\n",
    "for k, v in msg_poll.items():\n",
    "    print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
